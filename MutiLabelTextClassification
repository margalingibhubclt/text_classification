Large Scale Multi-label Text Classification With Deep Learning

Status: THIS IS DRAFT, WE ARE WORKING ON IT.THE TARGET IS TO WRITE A REPORT WITH PAPER STRCTURE, REACH LEVEL CLOSE TO PROJECT REPORT 
OF CS224N.

Abstract
Multi-label text classification is a more complex problem than single label text classification. Labels may have some internal relationship
or hierarchical structure between each other.We investigate serveral neural networks for multi-label classification. Our baseline is fastText,
a very simple model with n-gram features. 
We also explore two model with more complex structure: one is TextCNN, with model filters based on CNN; another is Hierarchical Attention
Networ, with hierarchical structure and attention mechanism.
These models are used for multi-label classification on Zhihu question & topic dataset. we show that deep models are model superior than 
shallow model.
f1 score for our baseline model is 0.72. two other model is around 0.80, increase around 11% compare to shallow model. by using simple
ensembling model of latter two models, performance improves for another 1.5%.

1.Introduction
Multi-label text classification is a complex natural language processing task which require to predict multi-label for one or serveral
sentences. One simple way to do this task is to cast this problem into single label text classification. 
Our baseline method is fastText. it is scalable,fast, shallow method which can train large scale of data in a few mintues, 
even with lots of classes for the label. another feature of this method is that it use n-gram features from dataset as its input. so it can
capture rich features. and the performance is fine.
However, this method is too simple, and may not complex enough or effective for modelling complex problem. to slove the problem, we explore 
convolutional neural network based model(TextCNN), and model with 
hierarchical structure and attention mechanism(Hierarchical Attention Network). from the experiement, we show that both models exceed the 
baseline more in a large margin.

2.Related Work
Problem Overview
Suppose we have n lables for one sentence. then we can construct n pairs of sentence-label as training data; during inference, we can just 
retrieve top-k labels for a sentence. However, this setting has obvious problem: the training data is inconsistent or have logical 
contradiction. This is something like you tell the model this picture is cat in the first second, and one second later you tell that it is 
also a dog. It will cause confuse to the model.
The second way to do multi-label text classification is to assume that each labels is independent. given a sentence, the model is ask to
maximize the prediction of each label independently. in another saying,
for each label the possibility will be from 0.0 to 1.0; and the loss is based on cross entropy between true distribution of labels and 
prediction distribution of labels.
In this eassy we use the later method. we will now descibe our baseline model and these two model.

3.Models
3.1 Baseline model: fastText

3.2 TextCNN

3.3 Hierarchical Attention Network

3.4 RCNN

4.Experiments
multi-label setting. report performance.
ensembel model.report performance.

5.Conclusion

Reference
1.Bag of Tricks for Efficient Text Classification

2.Convolutional Neural Networks for Sentence Classification

3.A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification

4.Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow, from www.wildml.com

5.Recurrent Convolutional Neural Network for Text Classification

6.Hierarchical Attention Networks for Document Classification

7.Neural Machine Translation by Jointly Learning to Align and Translate

8.Attention Is All You Need

9.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing

10.Tracking the state of world with recurrent entity networks

11.Ensemble Selection from Libraries of Models

12.Large Scale Multi-label Text Classiﬁcation with Semantic Word Vectors

13.CS224D FinalProject: Neural Network Ensembles for Sentiment Classiﬁcation
