Large Scale Multi-label Text Classification With Deep Learning

Status: THIS IS DRAFT, WE ARE WORKING ON IT.THE TARGET IS TO WRITE A REPORT WITH PAPER STRCTURE, REACH LEVEL OF CLOSE TO PROJECT REPORT OF CS224N.

Abstract
Multi-label text classification is a more complex problem than single label text classification. Labels may have some internal relationship or hierarchical structure between each other.
We investigate serveral neural networks for multi-label classification. Our baseline is fastText, a very simple model with n-gram features. 
We also explore two model with more complex structure: one is TextCNN, with model filters based on CNN; another is Hierarchical Attention Networ, with hierarchical structure and attention mechanism.
These models are used for multi-label classification on Zhihu question & topic dataset. we show that deep models are model superior than shallow model.
f1 score for our baseline model is 0.72. two other model is around 0.80, increase around 11% compare to shallow model. by using simple ensembling model of latter two models, performance improves for another 1.5%.

1.Introduction
Multi-label text classification is a complex natural language processing task which require to predict multi-label for one or serveral sentences. One simple way to do this task is to cast this problem into single label
text classification. 
Suppose we have n lables for one sentence. then you can construct n pairs of sentence-label as training data; during inference, you can just retrieve top-k labels for a sentence. However, this setting has 
obvious problem: the training data is inconsistent or have logical contradiction. This is something like you tell the model this picture is cat, and it is also a dog. 
Another way to do multi-label text classification is to assume that each labels is independent. given a sentence, the model is ask to maximize the prediction of each label independent. in another saying,
for each label the possibility will be from 0 to 1; and the loss is based on cross entropy between true distribution of label and prediction distribution of label.

2.Model
1)fastText,2)TextCNN,3)Hierarchical Attention Network,4)RCNN

3.Related Work

4.Experiments
multi-label setting. report performance.
ensembel model.report performance.

5.Conclusion

Reference
1.Bag of Tricks for Efficient Text Classification

2.Convolutional Neural Networks for Sentence Classification

3.A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification

4.Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow, from www.wildml.com

5.Recurrent Convolutional Neural Network for Text Classification

6.Hierarchical Attention Networks for Document Classification

7.Neural Machine Translation by Jointly Learning to Align and Translate

8.Attention Is All You Need

9.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing

10.Tracking the state of world with recurrent entity networks

11.Ensemble Selection from Libraries of Models

12.Large Scale Multi-label Text Classiﬁcation with Semantic Word Vectors

13.CS224D FinalProject: Neural Network Ensembles for Sentiment Classiﬁcation

.
